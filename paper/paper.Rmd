---
title: "Exploring a pragmatic account of Zipf's Law of Abbreviation using Bayesian data analysis"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Benjamin N. Peloquin} \\ \texttt{bpeloqui@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And
    {\large \bf Noah D. Goodman} \\ \texttt{noah@university.edu} \\ Department of Computer Science \\ Stanford University
    \And {\large \bf Michael C. Frank} \\ \texttt{mike@university.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    Zipf (1935) argued that the relation between a word's frequency and length was 
    a universal property of human languages emerging from the competing pressures to minimize speaker- and listener-effort during communication.
    More recent work has explored these ideas, providing additional evidence for a Zipfian analysis of this relation between frequency and length, what Zipf termed
    the "Law of Abbreviation". However, this work, including Zipf's original analysis, has primarily relied on large-scale observational data, 
    only providing indirect evidence for the causal role speaker- and listener-pressures have on the "Law of Abbreviation". 
    In an effort to isolate these competiting dynamics experimentally,
    Kanwal et al. (2017) conducted a study using an artificial language learning paradigm. The authors showed that 
    only when both a speaker- and listener-pressures were present did the Law of Abbreviation emerge. The authors
    highlight that while their data is consistent with a Zipfian analysis, it is also consistent with an alternative, pragmatic language-use account.
    We consider this latter account, conducting a Bayesian data analysis using the data from Kanwal et al. (2017) representing
    subjects as rational, pragmatic agents using the Rational Speech Act framework. Fitting subject-level parameters
    we find good fit between model posterior-predictive values and data from Kanwal et al. (2017). We argue this
    analysis provides evidence for a connection between Zipfian notions of efficiency driven language change and Gricean 
    notions of rational-pragmatic langauge use.
    
keywords:
    "Add your choice of indexing terms or keywords; kindly use a semi-colon; between each term."
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(png)
library(grid)
library(ggplot2)
library(xtable)
```

# Introduction

Zipf (1935) presented a view of human behavior that focused on *effort minimization* as a guiding principle. Relying on empirical evidence from corpora, he argued that the unique dynamics of speaker- and listener-effort minimization give rise to the distributional forms found  in natural language. Among these, Zipf examined the relation between a word's frequency in a corpus and a variety of properties including its rank-frequency, its denotation size, and its length. In terms of this last property, what he called the "Law of Abbreviation," Zipf claimed that the more frequent words are, the shorter they tend to be. Evidence for this relationship can be found across languages, may also be present in animal communication systems  (Ferrer-i-Cancho et, 2013), and has also been observed in artificial (programming) languages (CITATION from Kanwal).\par

Zipf argued that this relationship between frequency and length was an emergent property of competing speaker-listener pressures. Under this framing, communication is viewed as a cooperative, joint activity between interlocutors. However, this joint activity contains a fundamental asymmetry -- what is effortful for a speaker (production) is different from what is effortful for a listener (understanding).  An optimal speaker language, Zipf argued, would consist of a single, low-cost word which was fully ambiguous; referring to all possible objects in the world. Adopting such a language, a listener would need to disambiguate every utterance by the speaker. By contrast, if a language were optimised solely in terms of listener effort, the language should bijectively map all forms to meanings so there was no need for a listener to disambiguate.\par

Zipf argued that these competing forces, what he called “Speaker’s economy” and “Auditor’s economy”, give rise to the particular distributional forms found in all languages, including the relationship between word frequency and length. While this framework is compelling and can largely be seen as foundational to a family of theories examining efficiency in language-structure (CITATIONS) and  -use (CITATIONS), the issue is far from settled. Subsequent work has argued that properties such as the relation between word frequency and length can theoretically be explained by a process of random typing (Ferrer-i-Cancho & Mascoso del Pardo, 2012). Kanwal et al. (2017) highlight the fact that Zipf’s original work, as well as more recent studies analysing large-scale, observational corpora, “do not explicitly target the hypothesized role of communicative pressures…” \par

To gain traction on the question of the causal role of speaker-listener pressures, Kanwal et al. (2017) implemented an artificial language learning paradigm to isolate the pressures directly. In line with Zipf's original analysis, they hypothesized that only when *both* speaker- and listener- pressures are present in a communication setting would the "Law of Abbreviation" emerge. The authors adopted a miniature artificial language learning set-up in which participants learned a language consisting of three words, which could be used to refer to two items. Crucially, two of the words were longer (seven versus three characters) than a third word. The two longer words could only be used to refer to one of the two objects. The third word, an abbreviation of the longer forms, could be used to refer to either of the two objects (it was ambiguous).\par

Operationalizing production cost as a function to word length, they hypothesized that when given a choice between alternatives, subjects should prefer to use the shorter, less costly term to refer to a more frequently occurring object. Importantly, however, this choice should only appear when there was both a speaker- and listener-pressures present. Results indicated that speakers did tend to use the less costly form for the more frequent item when both pressures were present, but, importantly, not in conditions where only one or neither of the pressures were present.\par

While Kanwal et al. (2017) represents an important experimental step in confirming Zipfian notions about lexicon-level efficiency, the authors highlight an important and subtle dimension to these results. Zipf’s “Law of Abbreviation” is a lexicon-level theory. That is, it attempts to explain why particular word forms are mapped to particular meanings. However, their results are, in theory, consistent with a pragmatic language-use account. We refer the reader to their paper for a more detailed discussion, but include the essential elements here:\newline

*"There is a distinction between a language-user's mental representation of the lexicon, and the form-meaning mappings they actually produce in communication...the nature of the communication task in this experiment may have caused them to produce only the short form for one object and the long form for the other based on purely pragmatic considerations."*\newline

In other words, the underlying lexicon learned by subjects may have remained unchanged during the experiment -- a finding which, on its face, is at odds with the claim that this experiment demonstrates lexicon-level change. Rather, subjects may have *used* the lexicon pragmatically, preferring to map less costly forms to more frequent objects, while leaving the underlying lexicon unchanged. The authors’ highlight that because they only recorded participants’ actual language production they cannot distinguish between the lexicon-change account and pragmatic, language-use account.\par

The current project addresses the question of whether the patterns of production behavior observed in Kanwal et al. (2017) can be explained by a pragmatic, language-use  account. To do so, we conduct a Bayesian data analysis, modeling subjects as rational pragmatic agents using the Rational Speech-Act framework (RSA) (Frank & Goodman, 2017; Goodman & Frank, 2016). To preview our results we find good fit from model posterior-predictive values to Kanwal et al. (2017) participant data ($r^2=0.987$), over and above what would would be expected from a baseline model ($r^2=0.684$). We believe this provides compelling evidence in support of the alternative hypothesis outlined by Kanwal et al. (2017) -- that subjects may have been behaving pragmatically, rather than demonstrating real change at the level of language-structure. \par

The paper proceeds as follows. We describe the experimental set-up of Kanwal et al. (2017), their hypotheses and the alternative interpretation of their results. We then introduce our general approach to the Bayesian data analysis as well as the Rational Speech-act framework we will use to model subject’s pragmatic language use. We evaluate our results summarising the findings and highlighting dimensions of their analysis our current implementation does not yet capture. Following this work we return the question posed by Kanwal et al. (2017) of how pragmatic language use might bootstrap language change and offer an argument for how this type of process could in theory unfold. We end with a summary of our findings and next steps.\par

```{r plot-inferred-params, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "TEMP - NEEDS UPDATING Inferred subject-level parameter distributions. Moving clock-wise from the upper-right facet -- inferred alpha values for subjects, the recursive depth for each subject, the probability that subjects are paying attention during the experiment, the inferred cost / referrent frequency structure used by the subject."}
img <- png::readPNG("figs/inferred-params-plot.png")
grid::grid.raster(img)
```


# Experimental evidence of Zipf's Law of Abbreviation

Artificial language learning paradigms have proven to be a productive methodology for studying language change in the laboratory (or online via web experiments) (CITATIONS). In the typical set-up, participants are trained on a simple language -- a series of form-meaning mappings, often using nonce words with novel objects. During test, participant’s language producing is recorded, typically over a series of communication events or across multiple generations of participants accomplishing some task (CITATIONS). Subsequent analyses focus on how properties of the produced language change as it used over time participants.\par

Kanwal et al. (2017) adopt this basic framework -- participants learned a miniature artificial language and were subsequently asked to communicate with an interlocutor in a reference game setting. Following ideas first indicated by Zipf (1935), the authors hypothesized the presence or absence of pressures to communicate quickly (speaker pressure) and accurately (listener pressure)  should impact the resulting languages. Participants were assigned to one of four possible conditions which varied in the pressures that were present. Both speaker- and listener-pressures were present in the *combined* condition, only the listener pressure present in the *accuracy* condition, only the speaker pressure present in the *time* conding. They also tested a fourth *neither* condition in which neither speaker nor listener pressures were present. For the purpose of our current project, we do not report our results on the control, *neither* condition as behavior should be arbitrary.\par

For a comprehensive description of the experimental methodology we refer the reader to Kanwal et al. (2017), focusing on essential details in this section. During training participants learned labels for two novel objects. Importantly, one of the objects occurred three times as frequently than the other (appearing 24 vs 8 times). During training participants observed the objects at their relative frequencies as well as labels. Labels appeared either as a long-form (e.g. “zopudon” or “zopekil”) or short form (“zop”). Importantly, in this set-up “zop” was ambiguous between the two object referents. During test, participants were asked to participate in a communication game in which a “director” was shown an object and asked to transmit its label to the “matcher.” The “director” was always given a choice to use the long-form label or the ambiguous short-form. Crucially, to transmit the label to the matcher the director had to click on it, holding the mouse while each character appeared sequentially. In this setting, transmitting a longer-form required more time -- an elegant operationalization of speaker production cost. Listener cost was operationalized via a pressure for the matcher to correctly identify the referent.\par

The authors hypothesized that a preference to map the short form to the frequent object, but not the infrequent object, should emerge in the *combined* condition. Recall that in this condition  there was both a pressure to *communicate quickly* (subjects were incentivized based on their time) and accurately (subjects were incentivized based on their matching performance). By contrast, in the *time* condition, in which there was no pressure to communicate accurately (no listener was present), and only the speaker cost was present, participants should always prefer the shortened form, regardless of the referent. In the *accuracy* condition there was no cost difference between the shortened and lengthened forms, but a listener was present. In this condition, participants should always prefer to use the unambiguous, long-form labels to maximize matching performance. Results across all three conditions were largely consistent with these predictions -- only in the *combined* speaker-listener pressure condition did the Law of Abbreviation emerge via shortening of the label for the most frequent object, but not the infrequent object.

# A changing lexicon or pragmatic language use?

While these results are consistent with a Zipfian account of the Law of Abbreviation, Kanwal et al. (2017) highlight an alternative explanation. They describe a distinction between the "mental representation" of the lexicon and produced forms. That is, Zipf's Law of Abbreviation is an account of lexical change, not language-use. If participants are retaining both the long-forms and short-form word in their "mental lexicon," but *using the lexicon pragmatically*, then the lexicon itself has not "changed." This analysis falls largely in line with Gricean notion of natural language pragmatics.\par

Grice (1975) argued that communication could largely be understood as an instance of rational, cooperative behavior. Under this framework speakers choose utterances to convey meanings. Listeners attempt to infer the speaker's intended meaning given utterances. At the heart of Grice’s theory was a set of conversational maxims known to both speakers and listeners (be truthful, relevant, informative and perspicuous). Inferences about a speaker’s intended meaning could be derived from speaker behavior relative to these maxims. In particular, this framework assumes that the listener can reason about the speaker’s intentions and relevant contextual information (e.g. common ground).\par

Translating this framing to the current project, we assume that the “matcher” can use contextual information. For example, in the *combined* condition the matcher knows that it is mutually beneficial for them to complete the task as quickly and as accurately as possible. They know the short label is faster (less costly to the speaker) than using a long-form, and that the frequent object occurs three times more often than the infrequent object. In a given trial, in which the matcher is trying to complete the task a Gricean description of their reasoning might go as follows: \newline
*If the director is trying to transmit information as quickly as possible they should always use the faster, shortened form. But if they want me to pick out the correct referent they should always transmit the unambiguous, slower form. An optimal trade-off should use the short form in as many trials as possible without incurring undo ambiguity. Therefore we should map the shorter, ambiguous form to the more frequent object.*\newline

Notably, by reasoning about one-another, about the goals of the game (be quick, be accurate), and the details of context (differential word costs and object frequencies) a pair can converge on optimal *use* of the provided language *without changing the underlying structure*.


# Rational speech act theory as a model for pragmatic participant behavior

To model this type of rational, pragmatic interaction we adopt the Rational Speech Act Framework (Frank & Goodman, 2012; Goodman & Frank, 2016). RSA is a recursive Bayesian model of pragmatic language production and interpretation, which can largely be seen as a mathematical formalization of essential Gricean principles. RSA has proven to be a productive framework for modeling a range of pragmatic phenomena from hyperbole, metaphor, scalar implicature and others (CITATIONS). Under RSA a speaker agent describes a conditional distribution, mapping meanings $u\in U$ to utterances $m\in M$, written as $S(u|m)$. A speaker agent describes a conditional distribution mapping from utterances to meaning, written as $L(m|u)$. Importantly, each of these functions is described in terms of the other. That is,

\begin{equation}
S_i(u|m) \propto e^{-\alpha\times U(u;m)}
\end{equation}
Where
\begin{equation}
U(u;m) = -log(L_{i-1}(m|u)) - cost(u)
\end{equation}
And 
\begin{equation}
L_{i-1}(m|u) \propto S_{i-1}(u|m)\times p(m)
\end{equation}
Defining nested speaker and listener agents could, in principle, lead to infinite regress. We take a *literal listener*, denoted $L_0(m|u)$, as a base-case. The literal listener does not reason about a speaker model, rather this agent considers the literal semantics of the utterance.
\begin{equation}
L_0(m|u) \propto \delta_u(m)\times p(m)
\end{equation}
Where
\begin{equation}
\delta_u(m)=
\begin{cases}
1, & \text{if } m \in [[u]]\\
0, & o.w.
\end{cases}
\end{equation}

## Bayesian data analysis with RSA linking function

To assess the degree to which the results in Kanwal et al. (2017) can be described by a pragmatic, language-use account, we conduct a Bayesian data analysis, using RSA as our linking function. Our analysis proceeds in two parts. In the *parameter inference* phase we infer subject-level parameters. In the *model-checking and posterior prediction* phase we assess how well our model, with the inferred parameters, describes the experimental data.

### Parameter inference

```{r table-parameters, fig.env = "table", fig.pos = "H", fig.align='center', fig.width=3.2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "Parameters"}
img <- png::readPNG("figs/parameters-table.png")
grid::grid.raster(img)
```

We model each participant as an RSA agent, inferring for each the following set of parameters -- $\alpha$, $\beta$ are RSA model parameters corresponding the the subjects rationality and recursive depth, respectively. The parameters $\gamma$ and $\lambda$ correspond to experiment-specific parameters -- the particular condition (described by the utterance costs and object referent priors), and a focus parameter describing the probability that the subject is not paying attention and choosing to produce utterance randomly. Table 1 includes the parameters and their descriptions.\par

```{r plate-notation, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "THIS IS A PLACE-HOLD AND IS INCORRECT"}
img <- png::readPNG("figs/plate-temp.png")
grid::grid.raster(img)
```
For each subject we use $n=200$ samples using Metropolis-Hasting to approximate the posterior distribution over parameters. Figure 1. displays inferred subject params across the all participants.

### Posterior prediction and model checking

The posterior predictive distribution describes the data we should *expect to see* sampling from our fitted model. Intuitively if these predictions do not match the *actual* data used to fit the model, then we have a poor model. Recall that in the first step of our Bayesian data analysis we infer parameters for each subject including the rationality parameter ($\alpha$), their recursive depth ($\beta$),  the cost and referent condition structure ($\gamma$), and their level of attention ($\lambda$).\par

```{r plot-post-predictives, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.height=3, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Posterior predictive values lead to far better fit to human data than a baseline model. Horizontal axes display counts of short-form usage from subjects in Kanwal et al. (2017). Vertical axes display model predictions. The left facet displays predicted short-form usage under a baseline model that implements the 'optimal' solution for each condition. The right facet displays predicted short-form usage under our BDA model."}
img <- png::readPNG("figs/bda-vs-baseline-plot.png")
grid::grid.raster(img)
```

Having inferred these subject-level parameters we’d like to ``run” our RSA-participants back through the experiment. That is, for each participant and for each trial they saw, we sample an utterance (e.g. $u \sim S_{\text{subject}_i,\text{ trial}_j}(u|m)$). For example, for a participant in the *combined* condition we would sample an utterance given that the actual participant viewed some object on trial 1, repeating the same process for all 32 trials, 24 of which would require the model to refer to a frequent object and 8 requiring the model to refer to an infrequent object. Having simulated datasets for each participant we can compare the number of times the RSA-agent and actual participant used the short-form in each condition following the analysis in Kanwal et al. (2017).

#  Results

As a baseline we can compare a deterministic speaker using the optimal Zipfian language in each condition. Under this model a speaker in the *combined* condition should always map the short form the most frequent item, but not the infrequent item; in the *time* condition the speaker should always use the short form, regardless of the referent; in the *accuracy* condition the speaker should always use the long forms. This baseline describes a speaker who deterministically chooses an utterance based on the optimal strategy.

To assess model performance we correlate model posterior predictive values for our RSA-subjects and baseline mode to human data. Table 2 displays $r^2$ values for both models. We find close fit for our rational pragmatic agents, significantly above our baseline model.  Figure X shows the values.

\begin{tabular}{ |p{2cm}|p{1cm}|  }
 \hline
  Model & $r^2$ \\
 \hline
 Baseline & $0.397$ \\
 BDA RSA & $0.987$ \\
 \hline
\end{tabular}

### Limitations of the current analysis
While our current framework accounts for a large percentage of variance in the human data, there are aspects our current analysis does not capture. Figure 5. Of Kanwal et al. (2017) shows temporal effects throughout the experiment -- speakers are more likely to use the shorter form later in the experiment compared to earlier in the experiment. Our current model does not capture this particular effect. However, there are various ways we might try to instantiate temporal effects corresponding to this kind of convergence. In our current framework this might be instantiated via reduction in uncertainty over the prior frequency distributions for the objects referents. That is, uncertainty at the social-epistemic level -- does your partner have the same priors you do. Taking another angle we might assume that the cost of the long-form becomes more onerous as the task proceeds, making speakers even more likely to want to avoid it. Our current framework provides many opportunities for this type of follow-up.\par

  -- the temporal effects identified by the authors -- however, this remains to be a possible extension of the framework we introduce here. Moreover we frame our results in light  argue that this is largely in line with ideas put forth by the authors -- that pragmatic language use may be an essential precursor to more system-level language change. In their own words, "pragmatics-driven asymmetry in usage may or may not lead to an immediate shift in lexical representations, it may be an important first step in such a change."\par

# Connecting pragmatic language use to language change

# Conclusion

Languages around the world display a consistent relationship between word length and frequency -- more frequent words tend to be shorter (CITATIONS).  Zipf (1935) called this particular finding the “Law of Abbreviation” and characterized it in terms of competing speaker listener pressures. While other work has provided indirect evidence consistent with this Zipfian analysis (CITATIONS), Kanwal et al. (2017) attempted to derive this effect while isolating speaker and listener pressures experimentally. In an artificial language learning paradigm the authors showed that subjects were significantly more likely to produce lexicons consistent the “Law of Abbreviation,” but only when both speaker and listener pressures are present. While this evidence is compelling the authors highlight an alternative hypothesis that may have generated the same effect, but could not be distinguished in the current study -- pragmatic language use instead of lexical change. To test this secondary hypothesis we conducted a Bayesian data analysis, modeling participants as rational pragmatic agents using the Rational Speech-act (RSA) framework. Inferring subject-level parameters we are able to fit Kanwal et al. (2017) experimental data with high-levels of accuracy ($r^2=0.986$). While we believe this provides evidence for the pragmatic-language use interpretation we also highlight the compatibility of this interpretation with an additional insight of Kanwal et al. (2017) -- pragmatic language use may serve as an important bootstrap for future language change.

# Acknowledgements


Place acknowledgments (including funding information) in a section at
the end of the paper.

# References 

```{r}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
